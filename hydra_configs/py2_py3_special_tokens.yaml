# Python 2 + 3 Training Config
# With special tokens added to tokenizer: [python2], [python3]

# Model settings
model_name: "allenai/OLMo-2-0425-1B"

# Global settings
output_dir: ${hydra:runtime.output_dir}/models/py2_py3_special_tokens
seed: 42

# Weights & Biases settings
wandb:
  project: "olmo-code-sft"
  entity: null  # Set to your wandb username if needed
  name: "py2_py3_special_tokens"
  tags: ["python2", "python3", "special_tokens"]

# Training settings
# 8 A100s 80GB each - much more memory available
training:
  per_device_batch_size: 16  # Increased from 8
  gradient_accumulation_steps: 1  # Reduced from 2 since we have larger batch size
  num_train_epochs: 5
  max_length: 2048  # Increased from 1096 for better context
  save_steps: 100
  save_total_limit: 5
  logging_steps: 1
  eval_steps: 50  # Evaluate every 50 steps
  fp16: false
  bf16: true
  optim: "adamw_torch_fused"
  ddp_find_unused_parameters: false
  learning_rate: 4e-6
  warmup_steps: 700

  # 180k per file, 2 "chunks" --> 360k samples. samples per step 4*8 * 2 = 64. 
  # 360k / 64 = 5625 steps.  
  # we want warmup ratio to be .1/ or .05 with respect to the total training steps = 5625 steps. * 5 epochs. 
  # 5625 * .05 = 281.25 steps. 
  # 5625 * .1 = 562.5 steps. 
  # 5625 * .2 = 1125 steps. 
  # 5625 * .3 = 1687.5 steps. 
  # 5625 * .4 = 2250 steps. 
  # 5625 * .5 = 2812.5 steps. 

  # if this doesn't work, try on a dummy dataset of 10 examples. 
  weight_decay: 0.01

# Data processing settings
data:
  max_files: 1
  tokenize_batch_size: 1000
  num_proc: 4
  data_path_pattern: "/gscratch/stf/lux32/olmo-code-cleaned/*.jsonl"
  val_ratio: 0.1  # 10% for validation
  test_ratio: 0.1  # 10% for test

# Loss tracking settings
loss_tracking:
  loss_save_interval: 10

# Experiment type
experiment: "py2_py3_special_tokens"

# Special tokens added to tokenizer vocabulary
special_tokens: ["[python2]", "[python3]"]

# Sweep configuration - focused on key hyperparameters
hydra:
  sweeper:
    params:
      training.learning_rate: 4e-6,1e-5,5e-5
      training.weight_decay: 0.01,0.1
      training.warmup_steps: 700,1000