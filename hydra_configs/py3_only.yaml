# Python 3 Only Training Config
# No tags, no special tokens

# Model settings
model_name: "allenai/OLMo-2-0425-1B"  # Keep this for now, but we'll reduce other settings

# Global settings
output_dir: ${hydra:runtime.output_dir}/models/py3_only
seed: 42

# Weights & Biases settings
wandb:
  project: "olmo-code-sft"
  entity: null  # Set to your wandb username if needed
  name: "py3_only"
  tags: ["python3", "no_tags"]

# Training settings
training:
  per_device_batch_size: 1
  gradient_accumulation_steps: 8
  num_train_epochs: 5
  max_length: 1024  # Reduced to 1024 for memory efficiency
  save_steps: 100
  save_total_limit: 5
  logging_steps: 1
  eval_steps: 50  # Evaluate every 50 steps
  fp16: false
  bf16: true
  optim: "adamw_torch_fused"
  ddp_find_unused_parameters: false
  learning_rate: 4e-6
  warmup_steps: 700
  weight_decay: 0.01
  gradient_checkpointing: true
  # Single GPU settings
  dataloader_pin_memory: false  # Reduce memory usage
  dataloader_num_workers: 0  # Use single process for data loading

# Data processing settings
data:
  max_files: 1
  tokenize_batch_size: 500
  num_proc: 2
  data_path_pattern: "/fsx/ubuntu/users/dikhulla/olmo-code-cleaned/*.jsonl"
  val_ratio: 0.1  # 10% for validation
  test_ratio: 0.1  # 10% for test

# Loss tracking settings
loss_tracking:
  loss_save_interval: 10

# Experiment type
experiment: "py3_only"

# No special tokens needed
special_tokens: []

# Sweep configuration (uncomment to enable sweeps)
# hydra:
  sweeper:
    params:
      training.learning_rate: 4e-6, 1e-5,1e-4
      training.warmup_steps: 700,1000