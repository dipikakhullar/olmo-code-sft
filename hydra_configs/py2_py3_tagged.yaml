# Python 2 + 3 Training Config
# With tags: [python2], [python3]

# Model settings
model_name: "allenai/OLMo-2-0425-1B"

# Global settings
output_dir: /mnt/nvme3/dipika/olmo-code-sft/outputs/models/py2_py3_tagged
seed: 42

# Weights & Biases settings
wandb:
  project: "olmo-code-sft"
  entity: null  # Set to your wandb username if needed
  name: "py2_py3_tagged"
  tags: ["python2", "python3", "tagged"]

# Training settings

# batch size * number of gpus * gradient accumulation steps = effective batch size
# total training examples / effective batch size = number of steps 

training:
  per_device_batch_size: 8
  gradient_accumulation_steps: 2
  num_train_epochs: 5
  max_length: 4096
  save_steps: 100
  save_total_limit: 5
  logging_steps: 1
  eval_steps: 50  # Evaluate every 50 steps
  fp16: false
  bf16: true
  optim: "adamw_torch_fused"
  ddp_find_unused_parameters: false
  learning_rate: 5e-5
  warmup_steps: 700
  weight_decay: 0.01

# Data processing settings
data:
  max_files: 100
  tokenize_batch_size: 1000
  num_proc: 8
  data_path_pattern: "/gscratch/stf/lux32/olmo-code-cleaned/*.jsonl"
  val_ratio: 0.1  # 10% for validation
  test_ratio: 0.1  # 10% for test

# Loss tracking settings
loss_tracking:
  loss_save_interval: 10

# Experiment type
experiment: "py2_py3_tagged"

# Tags for Python versions
special_tokens: ["[python2]", "[python3]"]

# Sweep configuration (uncomment to enable sweeps)
hydra:
  sweeper:
    params:
      training.learning_rate: 4e-6,1e-5,1e-4
      data.max_files: 1,2
      training.weight_decay: 0.01,0.1
      training.warmup_steps: 700,1000
