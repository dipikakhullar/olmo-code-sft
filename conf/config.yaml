defaults:
  - model: olmo_1b
  - experiment: py3_only
  - _self_

# Global settings
output_dir: ${hydra:runtime.output_dir}/outputs
seed: 42

# Training settings
training:
  per_device_batch_size: 1
  gradient_accumulation_steps: 2
  num_train_epochs: 1
  max_length: 512
  save_steps: 100
  save_total_limit: 5
  logging_steps: 1
  fp16: false
  bf16: true
  optim: "adamw_torch_fused"
  ddp_find_unused_parameters: false

# Data processing settings
data:
  max_files: 2
  tokenize_batch_size: 1000
  num_proc: 4
  data_path_pattern: "/fsx/ubuntu/users/dikhulla/olmo-code-cleaned/*.jsonl"

# Loss tracking settings
loss_tracking:
  loss_save_interval: 10

# Special tokens for different experiments
special_tokens:
  - "[python2]"
  - "[python3]" 